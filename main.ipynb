{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b9797d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6 PDFs\n",
      "Example file: Attention Is All You Need.pdf\n",
      "First 500 characters:\n",
      " Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani∗\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Noam Shazeer∗\n",
      "Google Brain\n",
      "noam@google.com\n",
      "Niki Parmar∗\n",
      "Google Research\n",
      "nikip@google.com\n",
      "Jakob Uszkoreit∗\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jones∗\n",
      "Google Research\n",
      "llion@google.com\n",
      "Aidan N. Gomez∗†\n",
      "University of Toronto\n",
      "aidan@cs.toronto.edu\n",
      "Łukasz K\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Path to your data folder\n",
    "DATA_PATH = \"Data/\"\n",
    "\n",
    "# Function to extract text from PDFs\n",
    "def load_pdfs(data_path):\n",
    "    texts = []\n",
    "    for file in os.listdir(data_path):\n",
    "        if file.endswith(\".pdf\"):\n",
    "            doc = fitz.open(os.path.join(data_path, file))\n",
    "            pdf_text = \"\"\n",
    "            for page in doc:\n",
    "                pdf_text += page.get_text()\n",
    "            texts.append({\"file\": file, \"text\": pdf_text})\n",
    "    return texts\n",
    "\n",
    "# Load PDFs\n",
    "documents = load_pdfs(DATA_PATH)\n",
    "\n",
    "print(f\"Loaded {len(documents)} PDFs\")\n",
    "print(\"Example file:\", documents[0][\"file\"])\n",
    "print(\"First 500 characters:\\n\", documents[0][\"text\"][:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fecdab9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks created: 663\n",
      "Example chunk:\n",
      " Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani∗\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Noam Shazeer∗\n",
      "Google Brain\n",
      "noam@google.com\n",
      "Niki Par\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "all_chunks = []\n",
    "for doc in documents:\n",
    "    chunks = splitter.split_text(doc[\"text\"])\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        all_chunks.append({\n",
    "            \"file\": doc[\"file\"],\n",
    "            \"chunk_id\": i,\n",
    "            \"text\": chunk\n",
    "        })\n",
    "\n",
    "print(f\"Total chunks created: {len(all_chunks)}\")\n",
    "print(\"Example chunk:\\n\", all_chunks[0][\"text\"][:300])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "987204bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\Desktop\\The AI Engineers Program\\Arxiv-Bot\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\user\\Desktop\\The AI Engineers Program\\Arxiv-Bot\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\user\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All chunks stored in ChromaDB!\n",
      "Collection size: 663\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "chroma_client = chromadb.Client()\n",
    "\n",
    "embedding_func = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "    model_name=\"all-MiniLM-L6-v2\"  \n",
    ")\n",
    "\n",
    "collection = chroma_client.get_or_create_collection(\n",
    "    name=\"arxiv_papers\",\n",
    "    embedding_function=embedding_func\n",
    ")\n",
    "\n",
    "for chunk in all_chunks:\n",
    "    collection.add(\n",
    "        documents=[chunk[\"text\"]],\n",
    "        metadatas=[{\"file\": chunk[\"file\"], \"chunk_id\": chunk[\"chunk_id\"]}],\n",
    "        ids=[f'{chunk[\"file\"]}_{chunk[\"chunk_id\"]}']\n",
    "    )\n",
    "\n",
    "print(\"✅ All chunks stored in ChromaDB!\")\n",
    "print(\"Collection size:\", collection.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21bff2f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result 1:\n",
      "use publicly available data, making our work com-\n",
      "patible with open-sourcing, while most existing\n",
      "models rely on data which is either not publicly\n",
      "available or undocumented (e.g. “Books – 2TB” or\n",
      "“Social media conversations”). There exist some\n",
      "exceptions, notably OPT (Zhang et al., 2022),\n",
      "GPT-NeoX ( ...\n",
      "From: LLaMa.pdf\n",
      "\n",
      "Result 2:\n",
      "Transformer Language Models.\n",
      "Transformer (Vaswani et al., 2017) is a sequence-to-sequence\n",
      "architecture that makes heavy use of self-attention. Radford et al. (a) applied it to autoregressive lan-\n",
      "guage modeling by using a stack of Transformer decoders. Since then, Transformer-based language\n",
      "models h ...\n",
      "From: LoRA.pdf\n",
      "\n",
      "Result 3:\n",
      "the universal transformer [DGV+18]. Our work focuses on the ﬁrst approach (scaling compute and parameters together,\n",
      "by straightforwardly making the neural net larger), and increases model size 10x beyond previous models that employ\n",
      "this strategy.\n",
      "Several efforts have also systematically studied the  ...\n",
      "From: GPT-3.pdf\n"
     ]
    }
   ],
   "source": [
    "# Example query\n",
    "query = \"What is the main contribution of the Transformer model?\"\n",
    "\n",
    "# Search in ChromaDB\n",
    "results = collection.query(\n",
    "    query_texts=[query],\n",
    "    n_results=3\n",
    ")\n",
    "\n",
    "for i, doc in enumerate(results[\"documents\"][0]):\n",
    "    print(f\"\\nResult {i+1}:\")\n",
    "    print(doc[:300], \"...\")\n",
    "    print(\"From:\", results[\"metadatas\"][0][i][\"file\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e5b89da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model='mistral' created_at='2025-09-03T21:08:08.0035798Z' done=True done_reason='stop' total_duration=58944573700 load_duration=15315563600 prompt_eval_count=24 prompt_eval_duration=2362500400 eval_count=332 eval_duration=41255714700 message=Message(role='assistant', content=' Sure! Transformers are a type of model used in artificial intelligence, particularly in the field of natural language processing (NLP). They were introduced in a paper called \"Attention is All You Need\" by Vaswani et al., published in 2017.\\n\\nThe key concept behind Transformers is self-attention, which allows the model to focus on different words or parts of a sentence when generating an output, rather than simply considering each word sequentially. This makes Transformers more efficient and powerful compared to traditional recurrent neural networks (RNNs) for certain tasks, such as translation and language modeling.\\n\\nIn essence, Transformers work by using a mechanism called \"attention\" to weigh the importance of different words in the input sequence when generating an output. This is achieved through a process called \"scaling and squashing,\" where the attention scores are first scaled to a range between 0 and 1, then squashed into a usable value for each word.\\n\\nTransformers consist of several layers, each containing multiple \"attention heads.\" Each head processes the input sequence independently but shares weights across all heads, allowing the model to capture various aspects of the data simultaneously. The outputs from all heads are then concatenated and passed through a feed-forward neural network for final output prediction.\\n\\nTransformers have been extremely successful in various NLP tasks, such as machine translation, language modeling, and text classification. They are also the backbone of popular pretrained models like BERT (Bidirectional Encoder Representations from Transformers).', thinking=None, images=None, tool_name=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "response = ollama.chat(\n",
    "    model=\"mistral\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert AI/ML assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Explain Transformers in simple terms.\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e2699c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_arxiv_bot_with_sources(question, top_k=3):\n",
    "    if not question.strip():\n",
    "        return \" Please enter a question.\"\n",
    "    \n",
    "    # Retrieve top-k chunks\n",
    "    results = collection.query(\n",
    "        query_texts=[question],\n",
    "        n_results=top_k\n",
    "    )\n",
    "    \n",
    "    if not results[\"documents\"][0]:\n",
    "        return \" No relevant content found in the PDFs.\"\n",
    "    \n",
    "    context = \"\\n\\n\".join(results[\"documents\"][0])\n",
    "    \n",
    "    # Ollama chat\n",
    "    try:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert AI/ML assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Answer the question based ONLY on the context below. If not in the context, say 'I don't know'.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\"}\n",
    "        ]\n",
    "        response = ollama.chat(model=\"mistral\", messages=messages)\n",
    "        generated_text = response.message.content  # <-- extract text only\n",
    "    except Exception as e:\n",
    "        return f\" Error generating answer: {e}\"\n",
    "    \n",
    "    # Collect sources\n",
    "    sources = [md[\"file\"] for md in results[\"metadatas\"][0]]\n",
    "    \n",
    "    return f\"{generated_text}\\n\\n Sources: {', '.join(set(sources))}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1859fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_pdfs_to_collection(file_paths):\n",
    "    for path in file_paths:\n",
    "        # path is now a string path\n",
    "        doc = fitz.open(path)  \n",
    "        text = \"\"\n",
    "        for page in doc:\n",
    "            text += page.get_text()\n",
    "        chunks = splitter.split_text(text)\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            collection.add(\n",
    "                documents=[chunk],\n",
    "                metadatas=[{\"file\": os.path.basename(path), \"chunk_id\": i}],\n",
    "                ids=[f'{os.path.basename(path)}_{i}']\n",
    "            )\n",
    "    return \" PDFs added to collection!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b38fde79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7868\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7868/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"## Arxiv RAG AI Assistant\")\n",
    "    \n",
    "    # Upload PDFs (fixed)\n",
    "    pdf_input = gr.File(file_types=[\".pdf\"], type=\"filepath\", label=\"Upload new PDFs\", file_count=\"multiple\")\n",
    "    \n",
    "    # Question input\n",
    "    question_input = gr.Textbox(lines=2, placeholder=\"Type your question here...\", label=\"Ask a question\")\n",
    "    \n",
    "    # Answer output\n",
    "    answer_output = gr.Textbox(label=\"Answer\")\n",
    "    \n",
    "    # Buttons\n",
    "    upload_btn = gr.Button(\"Add PDFs\")\n",
    "    ask_btn = gr.Button(\"Ask Question\")\n",
    "\n",
    "    upload_btn.click(lambda files: add_pdfs_to_collection(files), inputs=[pdf_input], outputs=[answer_output])\n",
    "    ask_btn.click(\n",
    "    fn=ask_arxiv_bot_with_sources,  # must return a string\n",
    "    inputs=[question_input],\n",
    "    outputs=[answer_output]\n",
    ")\n",
    "\n",
    "    \n",
    "demo.launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
